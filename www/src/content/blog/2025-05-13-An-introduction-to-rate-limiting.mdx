---
title: "An introduction to Rate Limiting"
description: "The whats, whys, and hows of effective rate limiting."
slug: rate-limiting-intro
date: 2025-05-13
author: Aristo Spanos
tags:
  - hono
  - ratelimit
  - architecture
  - learning
---

Although rate limiting is essential for most production applications, it’s quite difficult to find comprehensive resources on rate limiting principles and patterns. Most articles or tutorials provide only a brief introduction to the why’s and how’s, including a list of common rate limiting algorithms, possibly accompanied by in-memory examples.

In part, this is because most rate limiting today—outside of enterprise-scale apps—is handled invisibly by cloud providers or is implemented using a third-party package. Nonetheless, it’s helpful to understand what motivates the design of different limiters. 

Some limiters are better-suited to a particular use-case than others, and properly configuring the tool you choose isn’t always obvious. Off-the-shelf limiters are also built to meet a generalized set of needs, so more complex requirements may demand a custom solution.

This is the first article in a series meant to illuminate this opaque and highly varied landscape. The series will bring together the theory and practice of rate limiting, using contextualized real-world examples. Implementations will target the [Hono](https://hono.dev/docs/) ecosystem, but most of the philosophy and design principles will be relevant regardless of the stack you choose.

This article won’t be especially technical, but it will be helpful to have a basic understanding of how HTTP requests work, as well as some of the business and technical requirements that influence backend development. I’ll introduce rate limiting in broad strokes, covering key concepts including:

- The **vulnerabilities** rate limiters are designed to defend
- **Identifiers** used to limit clients appropriately
- How to **store and manage data** tracking client consumption
- Effectively **communicating** rate limit info to clients

In subsequent articles, we’ll take a closer look at how specific requirements and constraints affect rate limiter design, and how different limiters can work together to secure an app. To avoid getting lost in the weeds, we’ll do so using practical examples that cover:

- Adding popular third-party rate limiters to a Hono app
- Rate limiting algorithms, with real-world Redis implementations
- Rolling your own rate limiting solutions

By the end of this series, you will be able to confidently pick the right limiter(s) for your use-case, or even roll your own! First though, let’s review the whats, whys, and hows of rate limiting.

## What is rate limiting anyway?

In practice, rate limiting refers to a broad variety of policies and implementations with equally-diverse goals. In principle though, these are united by a set of common factors. In brief, rate limiters control how often a resource can be accessed, and by whom. 

To accomplish this, most limiters use some kind of identifier (e.g., user ID or client IP) to track requests from a given source, and an algorithm to determine if and when requests from each source should be allowed.

The specifics vary by algorithm and implementation, but most limiters are defined by:

- A **limit** on how many requests can be made within a defined **window** of time
- The **cost** of each request counted against the limit
- How often a client’s limit is **refilled** or **reset**, allowing additional requests

Client consumption is measured in **tokens (**or as the sum of request timestamps), often stored in an in-memory database like [Redis](https://redis.io/) or [Memcached](https://memcached.org/). On each request, the rate limiting algorithm is used to determine whether the number of recent requests exceeds the limit.

In-memory databases are used to reduce the latency introduced by a rate limiting layer, but limiters inevitably increase response times—albeit minimally. They play a crucial role in capacity management and application security though, preventing the app from crashing under a flood of traffic, and mitigating an app’s vulnerability to certain attack vectors.

## Why rate limit at all?

We’ve all gotten a `429: Too many requests` error when building with third-party services, perhaps after spamming the service while developing a new feature. Most of us have also likely been locked out of an app (or device) after mis-entering the password one too many times. Broadly speaking, these represent the two main roles that rate limiting plays in our apps.

Rate limiting is most commonly deployed to manage service usage and capacity, but it also plays a vital role in auth workflows. Naturally, where and how this limiting is implemented depends greatly on the abuse vectors it guards against.

### Vulnerabilities

- **Resource Exhaustion:** An app’s ability to process requests is essentially finite. If too many requests come in at once, an unprotected system may crash, or auto-scale to the tune of tens of thousands of dollars.
    
    Floods of traffic may reflect an innocent spike in engagement—thousands of users flocking to buy tickets—or a [Denial of Service (DoS) attack](https://cheatsheetseries.owasp.org/cheatsheets/Denial_of_Service_Cheat_Sheet.html) intended to block legitimate requests or crash the app outright. In either case, rate limiting can be used to cap processed requests at a manageable (and affordable) level.
    
- **Resource Exploitation:** Even if an app can process all the traffic it’s receiving, it may not be serving clients equally or fairly. Especially in the case of paid services—where users expect a defined level of access—it’s vital to constrain how often clients can access resources.
    
    Without account-based rate limits, a minority of users can hog the app’s capacity—e.g., through high-volume scraping, causing other users’ requests to lag or fail. Users can also exceed the consumption rate they’re paying for, leading to a “hidden” loss of revenue.
    
    Note that “capacity” doesn’t refer only to how many requests a service can handle. Both malicious and innocent request patterns can over-consume other app resources, like the availability of merchandise on a digital marketplace (aka [Inventory Denial](https://owasp.org/www-project-automated-threats-to-web-applications/assets/oats/EN/OAT-021_Denial_of_Inventory)).
    
- **Account Hijacking:** Not all vulnerabilities are a matter of capacity. Rate limiting in auth workflows is primarily deployed against [Brute Force](https://cheatsheetseries.owasp.org/cheatsheets/Nodejs_Security_Cheat_Sheet.html#take-precautions-against-brute-forcing) and [Credential Stuffing](https://cheatsheetseries.owasp.org/cheatsheets/Credential_Stuffing_Prevention_Cheat_Sheet.html#introduction) attacks. Brute force attacks attempt to gain access to a system by repeatedly guessing a user’s password, while credential stuffing blindly plugs known credentials into different services, seeking instances of their reuse.

## Identifying Clients

In practice, apps often use a combination of limiters—each using a different identifier—to address these different vulnerabilities. IP-based limiters can protect pre-login workflows, for example, while location-based limits can help manage app capacity across regions.

To pick the right identifier for each use-case, it’s important to understand the strengths and limitations of each, as well as how these can support different requirements and strategies. As developers, we must also be mindful of the ethical and legal implications each identifier entails.

### Ethical Considerations

Storing personally-identifiable user data makes users vulnerable to doxxing, targeted cyber attacks, and government overreach. It also habituates users (and developers) to an ongoing erosion of digital privacy that threatens their safety and political freedom.

Laws like GDPR (EU) and CCPA (California) are meant to mitigate this threat by regulating the collection and storage of user data. This makes them useful guidelines for developing applications that safeguard user privacy.

Ultimately though, it is our responsibility as developers to build applications that are not just legally compliant, but also maintain a rigorous ethical standard. Fortunately, there are a few simple steps we can take to protect users and their privacy:

- Avoid storing data that can identify users or their location. In the context of rate limiting, this is chiefly their IPs and geolocation, though device fingerprints are sometimes used as well.
- When collecting or storing identifying data is necessary, clearly communicate to users what data is used and why.
- Obscure identifying data like IPs with one-way hashes (e.g., `HMAC`), and regularly clear stale records from the limiter database.

As with every engineering problem, ethical development is a matter of compromise. We can’t safeguard both the application and its users without dealing with at least some identifying data. This shouldn’t be seen as an excuse to cut corners though, but rather as a challenge to better understand rate limiting tools and how to deploy them responsibly.

### User IDs

By far the most accurate and secure identifier is a unique user or account ID, typically passed through authentication headers. A unique ID ensures that requests are counted against a specific account, making it possible to enforce tiered limits for different subscriptions.

Account or user IDs are the most broadly useful identifier, as they can be used to meet an app’s business needs, while also providing some protection against resource exhaustion and malicious request patterns. 

Note that while an ID uniquely identifies an *account*, it doesn’t inherently reveal anything about the user behind it or their location, making it a solid choice from an ethical perspective as well.

### Request IPs

Many examples use the client IP, as it’s relationship to requests is fairly intuitive, and the implementation doesn’t rely on the app’s business logic (e.g., for a user ID). IP limits are sub-optimal for capacity management though, as they do not necessarily correspond to unique users.

Multiple users could share the same IP, in which case one user could exhaust the limit for all. Conversely, a single client could also circumvent limits by accessing services using different IPs by using a VPN or a proxy.

While IPs should not be used as a primary limiter, they play a vital role in security-oriented rate limiting, especially in auth flows. If a user hasn’t yet been authenticated (or even registered), IP limits can prevent a malicious client from bombarding auth services, either to overwhelm them or in an effort to brute-force access. In more sophisticated rate limiting implementations, IPs with a pattern of malicious requests can also be blacklisted.

### Geolocation

Location-based limiting is far less common outside of enterprise-scale apps, perhaps in part because it is comparatively resource intensive. It can be used to dynamically manage limits by region based on expected traffic patterns. If request count decreases at night, for example, the rate limit for a given region could be lowered until the next morning.

This type of limiting is especially useful in a security context, as it can mitigate (and flag) suspicious request patterns. Doing so effectively requires some kind of geo-fencing though, as well as a service dedicated to managing the dynamic limits.

### Resource Limiting

Of course, it’s also possible to limit how much a resource is used at all, whether it’s a server, an  endpoint, or a single workflow. Since resource limiters don’t discriminate between clients, they’re not recommended for most rate limiting use-cases. They’re mostly useful for capacity management, and serve as a last resort against unexpected floods of traffic.

## Storing rate limit data

Since rate limiters depend on a record of recent requests, it’s not enough to simply identify clients. The chosen identifier is usually also used as a key to store and retrieve a record of how many requests the client has already made—typically within a given time frame.

There are a number of options for tracking client consumption. In-memory databases are popular though, as they reduce latency in the rate limit layer. Since some algorithms require multiple database calls, atomicity is another important consideration when choosing a storage solution.

Application architecture is arguably the most important factor though. Modern development patterns like distributed applications and edge functions introduce challenges and constraints that affect not just where data is stored, but also how it’s managed.

### (Local) Memory

Many rate limiting examples or bare-bones implementations simply track client requests locally, often using an object stored in memory. While this can be helpful during development, or for small apps running in a persisted environment, it isn’t a good solution at scale.

Local memory implementations break down in edge environments—where memory isn’t persisted long-term, and in distributed systems—where multiple app instances must be kept in sync. A local solution also means that the limiter and application logic share memory and compute, making scaling more difficult.

### Centralized

Using a centralized data store is a common alternative, as updates to the consumption record are available across requests and app instances.

While this strategy is a more stable and consistent alternative to a local `Map` or database, it introduces a request-processing bottleneck. A flood of requests from instances around the world can bog down the central store, causing even legitimate requests to lag or time-out.

In a distributed app, a centralized store can also add significant latency. After reaching the handler, each request must first be diverted to the limiter—possibly in a different region—before returning to the handling server for processing.

### Distributed

In the era of edge functions (and runtimes), it’s no surprise that distributed rate limiters are increasingly popular. This strategy involves colocating a limiter data store with each app instance, thereby substantially reducing latency and avoiding a single point of failure.

Of course, distributed rate limiters bring their own set of challenges. As with any distributed database, updates must be synchronized across instances. This ensures that users can’t bypass the limit simply by changing their “location” with a VPN. Special care must also be taken to mitigate race conditions, especially when using non-unique identifiers like IPs.

## Rate limiting strategies and implementations

Once you have a way to identify clients and keep track of their resource consumption, the next question is how precisely to limit requests. At the highest level, there are a few strategies to choose from, each applied to address different production concerns:

- Hard rate limiting: any requests that exceed the limit are rejected. This prioritizes security over user experience. You don’t want to give an attacker any wiggle room if they’re trying to brute-force a password, after all.
- Soft rate limiting: some excess requests are allowed, to allow for legitimate traffic bursts. If clients occasionally need a few extra requests, a softer limit can prevent service disruptions, though it’s vital to ensure that these bursts won’t overwhelm a resource’s capacity.
- Load shedding: lower-priority requests are rejected in favor of critical services. Reads might need to give way to writes, for example, if a service responsible for executing financial transactions is overwhelmed by an unexpected flood of traffic.
- Throttling: primarily used in auth workflows, this strategy gradually increases the time between allowed requests (typically after failed attempts) in order to mitigate attacks.

### Calculating when to allow or reject requests

Rate limiting algorithms essentially implement the chosen strategy. They are responsible for calculating how much of the limit a client has consumed, and when they are next eligible to make a request. They also periodically refill or reset the limit available to each client.

In the next article, we’ll cover rate limiting algorithms in greater depth, including real-world implementation. You’ll notice that some have quite a lot in common, or are even conceptually identical. These algorithms aren’t rigid formulas. Rather, they represent different ways to balance accuracy, performance, and complexity, while achieving essentially the same goal.

- **Fixed Window Counter:** Requests are counted within windows that have a fixed start and end time, and rejected if they exceed the limit. While simple and performant, fixed window limiters can be bypassed by concentrating requests at the window transition. By maxing out requests just before the current window expires, and just after the next begins, malicious clients can momentarily double the limit for a burst of requests.
- **Sliding Window Log:** Each request is recorded with a timestamp, and client consumption is measured by counting timestamps in a fixed-sized window relative to the current time. There’s no transition from one window to another, so this algorithm can’t be abused like the Fixed Window. This added accuracy comes at the cost of performance though, since an array of timestamps must be retrieved and counted.
- **Sliding Window Counter:** Essentially a hybrid of the previous two, this algorithm combines the count of requests in the previous and current windows to approximate the number of requests in a virtual sliding window. It is more performant than the Sliding Window Log, while retaining its invulnerability to abuse and window transitions. It’s not as accurate though, and can be more complicated to understand and implement.
- **Token Bucket:** Clients start with a maximum number of tokens, which is the upper limit for bursty traffic. Tokens are decremented on each request and refilled at a fixed rate, which controls the average request limit. The Token Bucket is popular due to the balance it strikes between simplicity, accuracy, and performance.
- **Leaky Bucket:** Essentially the Token Bucket’s inverse, the Leaky bucket begins empty. Tokens are added on each request, and drain at a fixed rate. It is most commonly used to process queued requests at a fixed rate, smoothing out bursty traffic.

## Monitoring and communicating rate limits

Setting limits, tracking consumption, and handling excessive requests is only half the battle though. Especially for apps with a larger user base, it’s vital to monitor resource use, along with request and rejection patterns. In the interest of user experience, it’s also necessary to share (at least some) information about limits with clients.

### Tuning limits and flagging attacks

Monitoring traffic and rate limiting has two primary goals: balancing capacity supply and demand, and flagging unusual (and possibly malicious) traffic patterns.

Effective rate limits are based in part on expected traffic, and must be adjusted in line with both micro and macro trends. Dynamic rate limits are often used to handle periodic variance in request frequency (e.g., daytime vs nighttime traffic), but may also respond to real-time fluctuations in traffic in order to handle irregular lulls and bursts.

Monitoring regular resource usage provides a baseline for both static and dynamic rate limits, which should be calibrated to account for changes in traffic over time. Tracking rejections and burst patterns can also yield significant insight. They may indicate that limits are improperly calibrated, but could also be triggered by a malicious agent.

### Keeping clients in the loop

In the simplest case, servers return a `429` error when a rate limit is exceeded. This isn’t especially helpful for recipients though, regardless of whether the client is an internal front-end, or an end-user. For this reason, both success and error responses often include rate limit information in their headers or bodies.

> Rate limiters protecting auth logic from attacks are a notable exception. In these cases it is recommended to share as little as possible. Users are expected to register or log in without exceeding limits, so any rejected requests are suspicious. Any details about the limit or when it resets make it easier to bypass limits, automate attacks, or avoid detection.

While there is some variety in implementation, many limiters will include the `Retry-After` header with `429` responses. The value—either in seconds or as an HTTP Date—indicates when the client may retry the request. This may be when the client’s quota is reset, or when it is expected to have been sufficiently refilled to afford another request.

The `RateLimit` header family is often used to provide additional detail, which can help clients avoid hitting the limit to begin with. This trades security for user experience, so it’s mostly used in responses to data requests or other business-limited operations.

- **`RateLimit-Limit`**: How many requests the client is allowed within a given window.
- **`RateLimit-Remaining`**: The number allowed of requests remaining (`limit - consumed`).
- **`RateLimit-Reset`**: Optionally, when the limit will be refilled or reset, in seconds or as a timestamp.

The best way to communicate rate limit data ultimately depends on the role the limiter is playing, as well as the app’s requirements. Third-party limiters may use different patterns or standards to communicate rate limit information, so it’s important to consider what needs to be shared (and with whom) when choosing a solution.

## From theory to praxis

As you can see, rate limiting is a complex and diverse space. There’s a lot that I didn’t cover, but some of those blanks will be filled in when we take a closer look at rate limiting algorithms and implementations in subsequent articles.

While not exhaustive, this summary of core rate limiting motivations and strategies is meant as a foundation for these deeper dives that follow. If you think I should have addressed something in greater depth—or if I missed it altogether—please let me know in the comments!

In the next article, we’ll implement common rate limiting algorithms using Redis, taking a closer look at how different strategies can be used to mitigate key vulnerabilities. If you’d like to get a head start, I’d recommend taking a look through [Upstash’s rate limiting package](https://github.com/upstash/ratelimit-js/), which I’ve found to be an incredibly helpful reference.